import pandas as pd
import numpy as np
from pathlib import Path
from pytz import timezone
import pytz
from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt
import seaborn as sns

# ================== å…¨å±€é…ç½® ==================
CONFIG = {
    "country_timezones": {
        'US': 'America/New_York',
        'CA': 'America/Toronto',
        'GB': 'Europe/London',
        'IN': 'Asia/Kolkata',
        'FR': 'Europe/Paris',
        'DE': 'Europe/Berlin'
    },
    "analysis_params": {
        "quantile_range": (1.5, 3.5),
        "target_retention": 0.8,
        "contamination": 0.05
    },
    "paths": {
        "input_dir": r"D:\Program Files\Codefield\CODE_Python\Projects\Kaggle_Trending YouTube Video Statistics\Selected_dataset",
        "output_dir": r"D:\Program Files\Codefield\CODE_Python\Projects\Kaggle_Trending YouTube Video Statistics\Processed_dataset",
        "final_output": r"D:\Program Files\Codefield\CODE_Python\Projects\Kaggle_Trending YouTube Video Statistics\Processed_dataset\merged_youtube_data.csv"
    }
}

# ================== æ ¸å¿ƒå·¥å…·å‡½æ•° ==================
def get_country_code(filename: str) -> str:
    """ä»æ–‡ä»¶åæå–æ ‡å‡†åŒ–å›½å®¶ä»£ç """
    base_name = Path(filename).name
    return base_name[:2].upper() if len(base_name) >= 2 else None

def convert_timezone(df: pd.DataFrame, country_code: str) -> pd.DataFrame:
    """æ—¶åŒºè½¬æ¢å¤„ç†"""
    if 'publish_time' in df.columns:
        try:
            df['publish_time'] = pd.to_datetime(df['publish_time'], utc=True)
            if country_code in CONFIG['country_timezones']:
                tz = timezone(CONFIG['country_timezones'][country_code])
                df['publish_time'] = df['publish_time'].dt.tz_convert(tz)
            df['publish_time'] = df['publish_time'].dt.strftime('%Y-%m-%d %H:%M:%S')
        except Exception as e:
            print(f"æ—¶é—´è½¬æ¢é”™è¯¯: {str(e)}")
    
    if 'trending_date' in df.columns:
        try:
            df['trending_date'] = pd.to_datetime(
                df['trending_date'], format='%y.%d.%m', errors='coerce'
            ).dt.strftime('%Y-%m-%d')
        except Exception as e:
            print(f"æ—¥æœŸè½¬æ¢é”™è¯¯: {str(e)}")
    return df

def normalize_boolean_columns(df: pd.DataFrame) -> pd.DataFrame:
    """å¸ƒå°”åˆ—æ ‡å‡†åŒ–"""
    bool_cols = ['comments_disabled', 'ratings_disabled', 'video_error_or_removed']
    for col in bool_cols:
        if col in df.columns:
            df[col] = df[col].astype(str).str.strip().str.lower()
            df[col] = df[col].map({'true': True, 'false': False, 'nan': None})
    return df

def basic_cleaning(df: pd.DataFrame) -> pd.DataFrame:
    """åŸºç¡€æ•°æ®æ¸…æ´—ï¼ˆåŒ…å«åˆ é™¤æŒ‡å®šåˆ—ï¼‰"""
    # åˆ é™¤æŒ‡å®šåˆ—
    columns_to_drop = ['thumbnail_link', 'comments_disabled', 
                      'ratings_disabled', 'video_error_or_removed']
    existing_cols = [col for col in columns_to_drop if col in df.columns]
    df = df.drop(columns=existing_cols, errors='ignore')
    print(f"å·²åˆ é™¤å†—ä½™åˆ—ï¼š{', '.join(existing_cols)}")

    # å»é‡å¤„ç†
    if all(col in df.columns for col in ['video_id', 'trending_date']):
        initial_count = len(df)
        df = df.drop_duplicates(subset=['video_id', 'trending_date'])
        print(f"å»é‡å¤„ç†: ç§»é™¤ {initial_count - len(df)} æ¡é‡å¤è®°å½•")
    
    # å¤„ç†ç¼ºå¤±å€¼
    required_cols = ['publish_time', 'trending_date', 'views', 'likes', 'comment_count']
    df = df.dropna(subset=[c for c in required_cols if c in df.columns])
    
    # å¡«å……è¾…åŠ©åˆ—
    for col in ['tags', 'description']:
        if col in df.columns:
            df[col] = df[col].fillna('')
    return df

# ================== æ™ºèƒ½è¿‡æ»¤æ¨¡å— ==================
def detect_outliers(df: pd.DataFrame, 
                   features: list = ['views', 'likes', 'comment_count'],
                   action: str = 'mark') -> pd.DataFrame:
    """åŸºäºå­¤ç«‹æ£®æ—çš„å¼‚å¸¸æ£€æµ‹ï¼ˆä¿®å¤åˆ—åˆ é™¤é—®é¢˜ï¼‰"""
    df = df.copy()
    valid_features = [col for col in features if col in df.columns]
    
    if not valid_features:
        raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„ç‰¹å¾åˆ—å¯ç”¨äºå¼‚å¸¸æ£€æµ‹")
    
    # æ•°æ®é¢„å¤„ç†ï¼ˆå¯¹æ•°å˜æ¢ï¼‰
    X = np.log1p(df[valid_features])
    
    # è®­ç»ƒå­¤ç«‹æ£®æ—æ¨¡å‹
    model = IsolationForest(
        n_estimators=100,
        contamination=CONFIG['analysis_params']['contamination'],
        random_state=42
    )
    model.fit(X)
    
    # é¢„æµ‹ç»“æœ
    scores = model.decision_function(X)
    pred = model.predict(X)
    
    # ç»“æœå¤„ç†ï¼ˆå§‹ç»ˆä¿ç•™å¼‚å¸¸è¯„åˆ†ï¼‰
    df['anomaly_score'] = scores  # ç¡®ä¿å§‹ç»ˆæ·»åŠ è¯¥åˆ—
    df['is_outlier'] = pred == -1
    print(f"æ£€æµ‹åˆ°å¼‚å¸¸å€¼æ•°é‡ï¼š{df['is_outlier'].sum()} ({df['is_outlier'].mean():.1%})")
    
    if action == 'remove':
        return df[~df['is_outlier']].drop(columns=['is_outlier'])  # åªåˆ é™¤æ ‡è®°åˆ—
    elif action == 'mark':
        return df
    else:
        raise ValueError(f"æ— æ•ˆæ“ä½œç±»å‹: {action}")

# ================== å¯è§†åŒ–æ¨¡å— ==================
def generate_country_report(df: pd.DataFrame, country_code: str):
    """ç”Ÿæˆå›½å®¶ç»´åº¦åˆ†ææŠ¥å‘Šï¼ˆå¢åŠ åˆ—å­˜åœ¨æ£€æŸ¥ï¼‰"""
    output_dir = Path(CONFIG['paths']['output_dir']) / 'visualizations'
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # å¼‚å¸¸è¯„åˆ†åˆ†å¸ƒï¼ˆå¢åŠ å®‰å…¨æ£€æŸ¥ï¼‰
    if 'anomaly_score' in df.columns:
        plt.figure(figsize=(10, 6))
        sns.histplot(df['anomaly_score'], bins=50, kde=True)
        plt.axvline(df['anomaly_score'].quantile(0.05), color='r', linestyle='--')
        plt.title(f'{country_code} Anomaly Score Distribution')
        plt.savefig(output_dir / f'{country_code}_anomaly_scores.png')
        plt.close()
    else:
        print(f"âš ï¸ è­¦å‘Šï¼šå›½å®¶ {country_code} æ•°æ®ç¼ºå°‘å¼‚å¸¸è¯„åˆ†åˆ—")

# ================== æ–‡ä»¶å¤„ç†æ¨¡å— ==================
def process_single_file(file_path: Path, output_dir: Path) -> Path:
    """å•æ–‡ä»¶å¤„ç†æµæ°´çº¿ï¼ˆè°ƒæ•´å¤„ç†é¡ºåºï¼‰"""
    df = pd.read_csv(file_path)
    country_code = get_country_code(file_path.name)
    
    if not country_code:
        print(f"è·³è¿‡æ— æ³•è¯†åˆ«çš„æ–‡ä»¶: {file_path.name}")
        return None
    
    print(f"\næ­£åœ¨å¤„ç†å›½å®¶: {country_code}")
    
    # æ‰§è¡Œå¤„ç†æµç¨‹
    df = convert_timezone(df, country_code)
    df = normalize_boolean_columns(df)
    df = basic_cleaning(df)
    
    # å…ˆç”ŸæˆæŠ¥å‘Šå†åˆ é™¤åˆ—
    temp_df = detect_outliers(df, action='mark')  # å…ˆæ ‡è®°ä¸åˆ é™¤
    generate_country_report(temp_df, country_code)
    
    # æœ€ç»ˆåˆ é™¤å¼‚å¸¸å€¼å’Œæ ‡è®°åˆ—
    processed_df = temp_df[~temp_df['is_outlier']].drop(columns=['is_outlier'])
    
    # ä¿å­˜ç»“æœ
    output_path = output_dir / f"{country_code}_processed.csv"
    processed_df.to_csv(output_path, index=False)
    print(f"å›½å®¶ {country_code} å¤„ç†å®Œæˆï¼Œä¿å­˜è‡³: {output_path}")
    return output_path

def enhanced_merge_all_files(output_dir: Path, final_path: Path):
    """å¢å¼ºç‰ˆåˆå¹¶å‡½æ•°"""
    print("\n" + "="*50)
    print("ğŸ“‚ å¼€å§‹åˆå¹¶é¢„å¤„ç†æ–‡ä»¶...")
    
    all_files = list(output_dir.glob("*_processed.csv"))
    print(f"ğŸ” æ‰¾åˆ°é¢„å¤„ç†æ–‡ä»¶æ•°é‡ï¼š{len(all_files)}")
    
    if not all_files:
        print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°ä»»ä½•é¢„å¤„ç†æ–‡ä»¶")
        return

    try:
        print("\n[ğŸ“¥ é˜¶æ®µ1/2] æ•°æ®åˆå¹¶")
        dfs = []
        total_raw = 0
        
        for f in all_files:
            print(f"â³ æ­£åœ¨åŠ è½½ï¼š{f.name}...", end="")
            df = pd.read_csv(f)
            dfs.append(df)
            total_raw += len(df)
            print(f" è®°å½•æ•°ï¼š{len(df):,} âœ”ï¸")
        
        merged_df = pd.concat(dfs, ignore_index=True)
        print(f"\nâœ… åˆå¹¶å®Œæˆï¼åŸå§‹æ€»è®°å½•æ•°ï¼š{total_raw:,}")

    except Exception as e:
        print(f"\nâŒ æ•°æ®åˆå¹¶å¤±è´¥ï¼š{str(e)}")
        return

    try:
        print("\n[ğŸ’¾ é˜¶æ®µ2/2] æœ€ç»ˆå¤„ç†ä¸ä¿å­˜")
        # å…¨å±€å¼‚å¸¸æ£€æµ‹
        final_df = detect_outliers(merged_df, action='remove')
        final_df.to_csv(final_path, index=False)
        print(f"âœ… å¤„ç†å®Œæˆï¼æœ€ç»ˆè®°å½•æ•°ï¼š{len(final_df):,}")

    except Exception as e:
        print(f"\nâŒ æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼š{str(e)}")

# ================== ä¸»æ‰§è¡Œæµç¨‹ ==================
def main():
    input_dir = Path(CONFIG['paths']['input_dir'])
    output_dir = Path(CONFIG['paths']['output_dir'])
    final_path = Path(CONFIG['paths']['final_output'])
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    processed_files = []
    for file_path in input_dir.glob("*.csv"):
        if result := process_single_file(file_path, output_dir):
            processed_files.append(result)
    
    if processed_files:
        enhanced_merge_all_files(output_dir, final_path)
    
    print("æ•°æ®å¤„ç†æµç¨‹å®Œæˆ")

if __name__ == "__main__":
    main()